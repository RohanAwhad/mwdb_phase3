# -*- coding: utf-8 -*-
"""Untitled62.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g87WOI90lYgqUyC7dUywds9N5zfXGfLJ
"""

# Imports
import torch
import torchvision.models as models
import torchvision
from torchvision import datasets
import csv
import gzip
import pandas as pd
import numpy as np

resnet50 = models.resnet50(pretrained=True) # Load pre-trained ResNet-50 model
data_dir = './caltech101_data' # Define the data directory where you want to save the dataset
caltech_dataset = datasets.Caltech101(data_dir, download=True) # Download and extract the dataset

from torchvision import transforms

preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])


def extract_features(image):
    image = image.convert('RGB')
    image = preprocess(image)
    image = image.unsqueeze(0)
    with torch.no_grad():
        features = resnet50(image)
    return features.cpu().numpy().flatten()

# Process even-numbered images grouped by labels
label_wise_features = {}
for i, (img, label) in enumerate(caltech_dataset):
    if i % 2 == 0:  # Check for even index
        if label not in label_wise_features:
            label_wise_features[label] = []
        features = extract_features(img)
        label_wise_features[label].append(features)

def pca(data, variance_threshold=0.95):
    # Standardize data
    data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)

    # Covariance matrix
    covariance_matrix = np.cov(data, rowvar=False)

    # Eigenvalues and eigenvectors
    eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)

    # Sort by eigenvalue in descending order
    sorted_index = np.argsort(eigenvalues)[::-1]
    sorted_eigenvalues = eigenvalues[sorted_index]
    sorted_eigenvectors = eigenvectors[:,sorted_index]

    # Compute cumulative variance
    cumulative_variance = np.cumsum(sorted_eigenvalues) / np.sum(sorted_eigenvalues)

    # Determine how many principal components to use
    num_components = np.where(cumulative_variance >= variance_threshold)[0][0] + 1

    return num_components

import os
class_names = sorted(os.listdir(os.path.join(caltech_dataset.root, "101_ObjectCategories")))
class_mapping = {i: name for i, name in enumerate(class_names)}

# Compute and print inherent dimensionality for each label
for label, features in label_wise_features.items():
    features_matrix = np.array(features)
    inherent_dimensionality = pca(features_matrix)
    label_name = class_mapping[label]  # Use the class_mapping here
    print(f"Label: {label_name}, Inherent Dimensionality: {inherent_dimensionality}")